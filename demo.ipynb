{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import string\n",
    "import tempfile\n",
    "import shutil\n",
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from torch import nn as nn\n",
    "from torch import Generator\n",
    "from torch.utils.data import random_split,DataLoader\n",
    "# TODO: Currently assumes that pytest runs from project root\n",
    "from util.car_dataset import check_dir, check_file, CarDataset, DatasetError\n",
    "from networks.common import *\n",
    "from networks.CNNbase import CNNBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def rand_string(length=5):\n",
    "    return ''.join(random.choices(string.ascii_letters +\n",
    "        string.digits, k=length))\n",
    "\n",
    "def loaded_dataset(temp_dir):\n",
    "    '''\n",
    "    Loads the car_dataset and returns the CarDataset object.\n",
    "    '''\n",
    "    #image_folder_list = ['0_Frames','1_Frames','2_Frames','3_Frames']\n",
    "    #label_file_list = ['0.txt','1.txt','2.txt','3.txt']\n",
    "    image_folder_list = ['0_Frames']\n",
    "    label_file_list = ['0.txt']\n",
    "    car_data = CarDataset(temp_dir,image_folder_list,label_file_list)\n",
    "    car_data.drop_nan()\n",
    "\n",
    "    return car_data\n",
    "\n",
    "def dataset_loader(loaded_dataset):\n",
    "    length = len(loaded_dataset)\n",
    "    split_set = random_split(loaded_dataset,[round(0.80*length)\n",
    "                ,round(0.20*length)],generator=Generator().manual_seed(42))\n",
    "    train_loader = DataLoader(split_set[0],batch_size=20)\n",
    "    test_loader = DataLoader(split_set[1],batch_size=20)\n",
    "    '''\n",
    "    Lowering values to sane levels to help run tests on github\n",
    "    runners. Ideally on local GPU (4GB) 80/20 split with batch size of\n",
    "    30 works well.\n",
    "    '''\n",
    "    return (train_loader,test_loader)\n",
    "\n",
    "def network():\n",
    "    network = CNNBasic()\n",
    "    network = network.to(get_device())\n",
    "    network.double()\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\Desktop\\dl-based-sensor-calib\n"
     ]
    }
   ],
   "source": [
    "root = os.getcwd()\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data = loaded_dataset(os.path.join(os.getcwd(),\"dummy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset\n",
    "datasets = dataset_loader(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Network\n",
    "net = network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:14,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0023945062209875597, 0.0021735814722538913, 0.0020342568510803363, 0.001790008499580588, 0.001741319400196628, 0.001776566849462381, 0.0018356273661992606, 0.001780277401897139, 0.0015712888014433994, 0.0014287431208034485, 0.0017053991203078431, 0.001633998732580986, 0.001277412105383961, 0.0014458903253205682, 0.0014783725855323545, 0.0009762948334857834, 0.001117353595888093, 0.0009775080117433415, 0.0011770719658377867, 0.0009869856737355894, 0.000899311560816988, 0.0007822256705433393, 0.00090340678836575, 0.0007801253008806608, 0.00096263780086569, 0.0007653563790641925, 0.0007641845554145912, 0.0007129695156406157, 0.0008104611854390814, 0.0006860670817180272, 0.0006220820009371797, 0.0007759756080550086, 0.0005339381539605198, 0.0005557450860693784, 0.0007455381600232448, 0.0007687409320583085, 0.0004971577505200862, 0.0006251108197948841, 0.0003951384427874184, 0.00044137909824217426, 0.0005875411301488106, 0.0005322422458625468, 0.0005609713665284566, 0.0004856531255728606, 0.00036191097975181734, 0.00044917043657988686, 0.00039314881853551255, 0.0003361850873200138]\n",
      "Train Epoch: 0\tMean Loss: 0.001022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train = datasets[0]\n",
    "print(\"Start Training\")\n",
    "run_training(net,train,1,get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Inference\n",
      "Test Set Average Loss 0.0017\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test = datasets[1]\n",
    "print(\"Start Inference\")\n",
    "run_inference(net,test,get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
