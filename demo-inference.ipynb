{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import string\n",
    "import tempfile\n",
    "import shutil\n",
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from torch import nn as nn\n",
    "from torch import Generator\n",
    "from torch.utils.data import random_split,DataLoader\n",
    "# TODO: Currently assumes that pytest runs from project root\n",
    "from util.car_dataset import check_dir, check_file, CarDataset, DatasetError\n",
    "from networks.common import *\n",
    "from networks.CNNbase import CNNBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def rand_string(length=5):\n",
    "    return ''.join(random.choices(string.ascii_letters +\n",
    "        string.digits, k=length))\n",
    "\n",
    "def loaded_dataset(temp_dir):\n",
    "    '''\n",
    "    Loads the car_dataset and returns the CarDataset object.\n",
    "    '''\n",
    "    image_folder_list = ['0_Frames','1_Frames','2_Frames','3_Frames']\n",
    "    label_file_list = ['0.txt','1.txt','2.txt','3.txt']\n",
    "    #image_folder_list = ['0_Frames']\n",
    "    #label_file_list = ['0.txt']\n",
    "    car_data = CarDataset(temp_dir,image_folder_list,label_file_list)\n",
    "    car_data.drop_nan()\n",
    "\n",
    "    return car_data\n",
    "\n",
    "def dataset_loader(loaded_dataset):\n",
    "    length = len(loaded_dataset)\n",
    "    split_set = random_split(loaded_dataset,[round(0.80*length)\n",
    "                ,round(0.20*length)],generator=Generator().manual_seed(42))\n",
    "    train_loader = DataLoader(split_set[0],batch_size=20)\n",
    "    test_loader = DataLoader(split_set[1],batch_size=1)\n",
    "    '''\n",
    "    Lowering values to sane levels to help run tests on github\n",
    "    runners. Ideally on local GPU (4GB) 80/20 split with batch size of\n",
    "    30 works well.\n",
    "    '''\n",
    "    return (train_loader,test_loader)\n",
    "\n",
    "def network():\n",
    "    network = CNNBasic()\n",
    "    network = network.to(get_device())\n",
    "    network.double()\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaded_test_dataset(temp_dir):\n",
    "    '''\n",
    "    Loads the car_dataset and returns the CarDataset object.\n",
    "    '''\n",
    "    #image_folder_list = ['0_Frames','1_Frames','2_Frames','3_Frames']\n",
    "    #label_file_list = ['0.txt','1.txt','2.txt','3.txt']\n",
    "    image_folder_list = ['4_Frames']\n",
    "    label_file_list = ['4.txt']\n",
    "    car_data = CarDataset(temp_dir,image_folder_list,label_file_list)\n",
    "    car_data.drop_nan()\n",
    "\n",
    "    return car_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/akshay.viswakumar/dl-based-sensor-calib\n"
     ]
    }
   ],
   "source": [
    "root = os.getcwd()\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Network\n",
    "net = network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Checkpoint\n",
    "epoch,loss =  load_model(net,net.optimizer,os.path.join(root,\"checkpoint-0.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Till Epoch : 0\n",
      "Training Loss : 0.0015060330042615533\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained Till Epoch : {}\".format(epoch))\n",
    "print(\"Training Loss : {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = loaded_test_dataset(os.path.join(root,\"dummy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_loader = DataLoader(new_test,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num = 1109\n",
      "Len = 1109\n",
      "Test Set Average Loss 0.1197\n"
     ]
    }
   ],
   "source": [
    "run_inference(net,new_test_loader,get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num = 1\n",
      "Len = 1\n",
      "Test Set Average Loss 0.0001\n",
      "Real = tensor([[0.0128, 0.0555]], device='cuda:0', dtype=torch.float64)\n",
      "Pred = tensor([[0.0224, 0.0653]], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Single inference\n",
    "samp = new_test[0]\n",
    "out = run_inference_single(net,samp,get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
